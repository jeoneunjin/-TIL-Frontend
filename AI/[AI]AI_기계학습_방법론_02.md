# 🤖 AI & 기계학습 방법론(2)

## 📚 목차

---

## 신경망모델

## 1-1. 단순(1D) 선형모델 : 모수적 함수

## 2. Shallow 네트워크

## 2-1. Shallow 네트워크 vs 1D 선형회귀

## 2-2. Shallow 네트워크 : 활성화 함수

## 2-3. Shallow 네트워크 : 모수

> 훈련데이터가 주어지면 손실함수를 정의하고, 손실을 최소화하는 모수를 찾는다.

## 2-4. Shallow 네트워크 : piecewise linear 함수

> 입력 구간을 나눠 조각별 선형 함수를 만듦(모수에 따라서 달라진다)

## 2-5. Shallow 네트워크 : Hidden Units

## 2-6. Shallow 네트워크 : 각 단계별 계산

1. 선형 변환
2. 활성화 함수 적용
3. 활성화 후 세타j배 적용
4. 나누었던 g,h,i 함수의 합 = 결과
   ? 4번 결과에서 꺾인 부분의 개수 3개 = Hidden Units 수 3개 우연인가?
   -> NO! 서로 연관있다. hidden units 수가 3개였기 때문에 결과에서 꺾이는 부분이 3개가 생긴 것

## 2-7. 네트워크 도식화

> 각 파라미터는 (출발 노드의 값) \* (가중치)를 출력 노드에 더한다

---

## 3. Shallow 네트워크의 표현력

## 3-1. 더 많은 Hidden Unit 가능

> 꼭 3개일 필요 X, 더 많은 히든 유닛 가능

## 3-2. Hidden Uit을 많이 두면

> 충분히 많은 Hidden Unit이 있다면, 임의의 1차원 함수를 원하는 정확도로 근사할 수 있다

---

## 4. 다중 출력/입력

## 4-1. 2개 출력의 네트워크

> 1 input, 4 hidden units, 2 outputs

## 4-2. 2개 입력의 네트워크

> 2 input, 3 hidden units, 1 outputs

## 4-3. 2개 입력 네트워크 : 단계별 계산

1. 선형 변환
2. 활성화 함수 적용
3. Hidden unit에 상수배 적용
4. 3개의 함수의 합

## 4-5 임의의 개수의 입력, Hidden Unit, 출력

---

## 5. Deep 네트워크

## 5-1. 2개의 네트워크를 하나로 합성

## 5-2. 합성 네트워크의 층(layer)별 출력

> x - 네트워크1 layer-> y - 네트워크2 layer-> y'

x에 따라 y'가 어떻게 달라지나?

## 5-3. "접기" 비유

## 5-4. Shallow 네트워크와 Deep 네트워크 비교

- Deep
  - 파라미터 수 : 20개
  - 조각적 선형 구역 : 9개 이상
- Shallow
  - 파라미터 수 : 19개
  - 조각적 선형 구역 : 7개 이하

=> 의미하는 것?

> 파라미터 수가 거의 같은데도,
> 깊은 네트워크가 더 많은 “조각적 선형 구역”을 만들어냄 → 더 복잡한 비선형 함수를 표현 가능.

---

## 6. Deep 네트워크 수식 표현

## 6-1. 2개의 네트워크를 하나로 합성 : 복잡한 수식

## 6-2. 2개의 네트워크를 하나로 합성 : 새로운 변수 활용

> 두 번째 네트워크의 hidden units에 새로운 파라미터 사용으로 수식의 단순화

## 6-3. 2층 네트워크의 표현

> x - 네트워크1 layer-> y - 네트워크2 layer-> y'
> 원래 이 형태를 x -> h1,h2,h3 -> h'1,h'2,h'3 -> y'로 중간 단계(y)없이 바로 연결이 가능

- x -> h1,h2,h3 : 출력 3개를 갖는 1-layer 네트워크로 볼 수 있음

## 6-4. 2층 네트워크 : 단계별 계산

1. 선형 변환
2. 활성화 함수 적용
3. Hidden unit에 상수배 적용
4. 3개의 함수의 합

---

## 신경망 적합(Fitting)

## 1. 선형회귀 예시

## 1-1. 손실함수

> 모델이 얼마나 잘못 예측하는지를 측정하는 함수

- 값이 작을수록 모델이 더 정확하게 학습되었다는 의미

## 1-2. 학습

> 손실함수를 최소하하는 파라미터를 찾음

## 1-3. 1D 선형회귀 예

- 선형회귀 모델에서 손실함수는 MSE(최소제곱 손실함수)
- 학습의 목적 : 손실함수를 최소화하는 직선 찾기

## 1-4. 1D 선형회귀 학습

- 등고선: 손실 함수 값의 크기
  - 밝을수록 손실이 큼
  - 어두울수록 손일이 적음
- 데이터와 선형함수 직선 : 데이터와 선의 오차가 크다면 손실값이 큼

  - 주황색 점 : 실제 데이터
  - 초록색 직선 : 현재 파라미터로 만든 모델

- 경사하강법(gradient descent)
  > 손실 함수의 값이 **줄어드는 방향**으로 파라미터를 **이동**하는 과정

## 3. 경사 하강법

## 3-1. 경사 하강법 알고리즘

> 손실 함수를 최소화하기 위해 파라미터 세타를 반복적으로 갱신하는 알고리즘

- 파라미터 갱신:
  - 기울기(미분값)의 반대방향으로 이동해야 손실 함수가 줄어듦
  - 알파>=0는 학습률로, 한 번의 스텝에서 이동하는 크기를 결정

## 3-2. 단계별 계산

1. 미분값 구하기
2. 파라미터 업데이트
   - 손실이 가장 빠르게 증가하는 방향의 반대 방향으로 이동해야 함(손실 최소화)
     (1,2번 반복)

## 3-4. 함수에 따른 최적화 난이도

- Convex :
  > 곡선이 항상 U자처럼 아래로 볼록. 그래프 위 임의 두 점을 잇는 직선이 그래프 위(또는 같은 위치)로 있음
- Non-convex :
  > 봉우리,골짜기, 오목한 구간이 섞인 모양. 두점을 이은 직성이 그래프 아래로 내려가는 구간이 생김
- Convex vs Non-convex 최적화 문제
  - 손실 함수 모양에 따라 최적화의 난이도가 달라짐
  1. Convex 문제 : 전역 최소값이 유일함 -> 최적화가 쉬움
  2. Non-convex 문제: 여러 개의 지역 최소값 또는 saddle 점이 있음 -> 최적화 어려움

## 4. 확률적 경사 하강법

## 4-1. 경사 하강법 vs 확률적 경사 하강법

- 경사 하강법의 단점

1. Non-convex 문제에서 지역 최소점에 빠지기 쉬움
2. 매 스텝마다 전체 데이터에 대한 미분값을 구하여 업데이트함
   - 스텝별 계산양이 많음

=> 대안 : 전체 데이터를 한번에 쓰는 대신, 무작위로 선택한 데이터 샘플 사용; 확률적 경사 하강법!

## 4-2. 확률적 경사 하강법의 개념

> 무작위 확률로 샘플된 일부 데이터(batch)만 사용하여 기울기 계산

## 4-4. 확률적 경사 하강법의 특성

- 무작위 샘플 데이터를 활용한 미분으로 경로의 무작위성이 있음
- local 최소점에서 빠질 위험이 상대적으로 적음

- 전체 데이터가 아닌 일부 배치로 기울기를 계산하기 때문에 노이즈가 섞여있음
  - 노이즈가 있지만 여전히 타당한 업데이트!
- 계산 비용 절감
  - 전체 데이터셋에서 작은 배치만 사용하므로 반복당 연산량이 적음
- full batch처럼 매끄럽게 수렵하지 않고, 무작위성 때문에 더 많이 진동(jitter, 지그재그)하면서 움직임
  - convex 문제에서는 full-batch 경사하강보다 수렴이 늦을 수 있음

## 5. 역전파(Backpropagation)

## 5-1. 네트워크 파라미터의 미분

? 네트워크 파라미터의 미분값은 어떻게 구할까?
-> layer별로 파라미터들이 존재함. 서로 영향을 미침

## 5-2. 연쇄 법칙

- 파라미터는 오메가를 갖는 합성 함수를 미분하면?
- 핵심 아이디어 ; 합성함수의 변화율 = 바깥의 변화율 \* 안쪽의 변화율
- 해석 :
  - f1의 입력(=f0)에 대한 기울기를 구한 뒤,
  - 그 값을 f0의 파라미터 오메가에 대한 기울기와 곱한다.
  - x는 파라미터가 아니므로 y'/x'는 여기서 필요 없음

## 5-3. 역전파

> 출력 오차를 기준으로 그래프를 거꾸로 따라가며 연쇄법칙으로 각 노드의 미분값을 계산하는 절차

1. 각 단계별 계산을 분해
2. 각 layer별 값을 계산
3. 각 layer별 값에 대한 출력 손실의 미분을 구함
