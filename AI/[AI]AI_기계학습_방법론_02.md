# 🤖 AI & 기계학습 방법론(2)

## 📚 목차

1. [신경망모델](#1-신경망모델)
2. [Shallow 네트워크](#2-shallow-네트워크)
3. [Deep 네트워크](#3-deep-네트워크)
4. [신경망 적합(Fitting)](#4-신경망-적합fitting)
5. [경사 하강법(Gradient Descent)](#5-경사-하강법gradient-descent)
6. [확률적 경사 하강법(SGD)](#6-확률적-경사-하강법-sgd)
7. [역전파(Backpropagation)](#7-역전파backpropagation)

---

## 1. 신경망모델

### 1-1. 단순(1D) 선형모델

> 모수적 함수(parametric function)  
> 한 개의 입력을 선형 함수로 예측하는 모델

$$
y = \beta_0 + \beta_1 x
$$

---

## 2. Shallow 네트워크

### 2-1. Shallow vs 1D 선형회귀

- Shallow 네트워크 = 입력 → Hidden Unit → 출력
- Hidden Unit 수에 따라 표현력 달라짐

### 2-2. 활성화 함수

- Hidden Unit에 비선형 함수 적용 (ReLU, Sigmoid 등)
- 입력 공간을 조각(piecewise) 선형 함수로 나눠 표현

### 2-3. 모수

- 훈련 데이터 주어지면, 손실함수를 최소화하는 모수를 찾음

### 2-4. Hidden Units와 표현력

- Hidden Units 수 = 꺾이는 구간 수
- 충분히 많으면 임의 1차원 함수 근사 가능

### 2-5. 단계별 계산

1. 선형 변환
2. 활성화 함수 적용
3. Hidden Unit 값 조정
4. 합산 → 최종 출력

---

## 3. Deep 네트워크

### 3-1. Shallow vs Deep 비교

| 구분    | 파라미터 수 | 조각적 선형 구역 |
| ------- | ----------- | ---------------- |
| Shallow | 19          | ≤ 7              |
| Deep    | 20          | ≥ 9              |

> 거의 같은 파라미터 수지만, Deep 네트워크가 더 복잡한 비선형 함수 표현 가능

### 3-2. 2층 네트워크 표현

- x → Hidden Layer1 → Hidden Layer2 → y'
- 각 층마다 선형 변환 + 활성화 함수 적용

---

## 4. 신경망 적합(Fitting)

### 4-1. 손실함수

> 모델의 예측 오차를 측정하는 함수

- 값이 작을수록 모델 정확도 ↑
- 예: MSE(선형회귀), Cross-Entropy(분류)

### 4-2. 학습

- 손실 최소화 → 최적 파라미터 추정
- 등고선 시각화 가능: 밝을수록 손실 ↑, 어두울수록 손실 ↓

---

## 5. 경사 하강법(Gradient Descent)

### 5-1. 알고리즘

1. 손실 함수 미분값 계산
2. 기울기 반대 방향으로 파라미터 이동
3. 학습률(α)로 이동 크기 조절
4. 반복 → 손실 최소화

### 5-2. Convex vs Non-convex

- Convex: 전역 최소값 유일 → 최적화 쉬움
- Non-convex: 여러 지역 최소점 존재 → 최적화 어려움

---

## 6. 확률적 경사 하강법 (SGD)

### 6-1. 개념

- 전체 데이터가 아닌 일부(batch)만 사용
- 기울기 계산에 무작위성 추가 → local 최소점 탈출 용이
- 계산 비용 절감, 반복당 연산량 ↓

### 6-2. 특징

- 노이즈가 있지만 여전히 유효한 업데이트
- full-batch 대비 진동(jitter) 존재 → 일부 convex 문제에서 수렴 느림

---

## 7. 역전파(Backpropagation)

### 7-1. 네트워크 파라미터 미분

- 각 층별 파라미터는 서로 영향을 주므로 **연쇄법칙** 적용 필요

### 7-2. 연쇄 법칙

$$
\frac{d(f \circ g)}{d\theta} = \frac{df}{dg} \cdot \frac{dg}{d\theta}
$$

- 합성 함수 미분 = 바깥 변화율 × 안쪽 변화율

### 7-3. Backprop 절차

1. 출력 오차 계산
2. 그래프 역방향으로 각 노드 미분값 계산
3. 각 층별 파라미터 업데이트

---

## 📌 요약

| 개념               | 특징                                        |
| ------------------ | ------------------------------------------- |
| Shallow 네트워크   | Hidden Units 수 = 꺾이는 구간 수            |
| Deep 네트워크      | 더 많은 층 → 복잡한 비선형 함수 표현        |
| 경사 하강법        | 손실 최소화 → 파라미터 업데이트             |
| 확률적 경사 하강법 | 일부 샘플 사용, 계산량 ↓, 노이즈 존재       |
| 역전파(Backprop)   | 연쇄법칙 적용, 출력 오차 기반 파라미터 미분 |

✅ 핵심 포인트:

- Hidden Units 수와 네트워크 깊이에 따라 표현력 결정
- Convex vs Non-convex에 따라 최적화 난이도 차이
- SGD와 Backprop으로 효율적 학습 가능
