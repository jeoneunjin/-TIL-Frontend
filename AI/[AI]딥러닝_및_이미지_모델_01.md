# 🤖 딥러닝 및 이미지 모델(1)

## 📚 목차

- ***

---

## 1. CNN

### 1-1. CNN 모델

### 합성곱 레이어(Convolution Layer)

- 입력 이미지를 필터와 연산하여 특징 맵(feature map)을 뽑아내는 모듈
- 1차원 구조로 변환하는 FCN과 달리 3차원 구조를 그대로 보존하면서 연산

#### Filter

> ! 차원을 반드시 Filter는 항상 입력의 깊이/채널 축과 동일한 차원이어야 함 !

- Filter 개수 = 출력 깊이

- 출력 해상도 = 입력 해상도 - 필터 해상도 + 1
  ? 만약 입출력 해상도를 유지하고 싶으면 ?
  -> 출력값 중 정의되지 않은 경우, 0 혹은 가장 가까운 출력값으로 대체(패딩)

* 바이어스도 추가

---

### 1-2. CNN 모델 구조

#### 중첩

선형 layer로 중첩한 구조는 하나의 레이어에 준한다. -> 별다른 결과를 만들진 못함
=> 비선형 layer의 중첩 필요

#### 필터의 의미

- 필터 시각화
  학습된 필터 시각화를 통해 각 모델(구조)가 학습한 정보를 이해 가능

#### 수용 영역

#### 풀링

#### 스트라이드 합성곱 - 풀링 한계 개선

- 일반 합성곱 vs 스트라이드 합성곱
  - 일반 합성곱 : 필터를 1칸씩 이동하면서 연산 수행
  - 스트라이드 합성곱 : 필터를 스트라이드 값만큼(S칸) 이동한 후 출력 연산
- 효과 :
  - 풀링은 학습 상수가 없지만 스트라이드는 커널을 동시에 학습
  - 해상도 저하로 인한 정보 손실이 적고, 풀링 + 합성곱을 하나의 레이어로 대체함
  - 고도화된 CNN에서는 스트라이드 합성곱을 풀링 대신 활용되는 경향

---

## 2. CNN 기반 모델 변천사

### 2-1. AlexNet(2012~)

> 5개의 합성곱 계층과 3개의 완전연결 계층으로 구성된 8계층 CNN 모델

- 모델 구조 특징
  - 5개의 합성곱 레이어 + 맥스 풀링 + 3개 연결층 레이어 + ReLU 비선형 활성화 => 8-레이어
- 입출력
  - 입력 : 이미지 / 3 x 277 x 277
  - 출력 : 레이블 / 1 x 1024 벡터

---

### 2-2. VGGNet(2014~)

> 5개의 합성곱 불록 + 맥스 풀링 구조

- 모델 구조 특징
  - 5개의 합성곱 레이어(각 블록당 합성곱 - ReLU 구조가 반복)
  - **각 블록당 맥스 풀링**
  - 3개 연결층 레이어
  - ReLU 비선형 활성화
  - **총 16개의 합성곱/연결층 레이어**
  - **최종 소프트맥스 수행**
- 입출력
  - 입력 : 이미지 / 3 x **244 x 244**
  - 출력 : 레이블 / 1 x 1024 벡터

#### AlexNet과의 비교

- 연산량 비교
  - AlexNet에서는 레이어별 연산량 차이가 매우 큼
  - VGG는 해상도를 줄일 때 마다 채널을 늘려 레이어별 연산량 차이를 줄임(분산하여 연산 처리)
  - AlexNet(0.7GFLOPs) vs VGG-16(13.6GFLOPs) 약 19.4배
- 메모리 비교
  - AlexNet(1.9MB) vs VGG-16(48.6MB) 약 25배
- 모델 상수 비교
  - AlexNet(61M) vs VGG-16(148M) 약 2배

#### 모델 특징

- 단순함 + 깊이의 강력한 성능을 보임
- 단순 설계로 모델 해석에 이상적
- 특징 추출기, 전이학습에 강력한 베이스라인(파라미터가 많고 연산량 요구가 매우 큼)

---

### 2-3. ResNet

> 합성곱 블록(VGG 유사)과 잔차(Residual) 블록
> CNN 기반 구조 중 가장 활발하게 활용

- 모델 구조 특징 :
  - 합성곱 블록과 잔차 블록
  - 잔차 블록은 블록입력을 그대로 출력에 더해주는 지름길 연결이 특징
  - Inception 모델에서 차원 축소로 활용된 1x1 합성곱을 적용, 연산 효율 개선
- 입출력
  - 입력 : 이미지 / 3 x **244 x 244**
  - 출력 : 레이블 / 1 x 1024 벡터

#### 등장 배경

- 배치 정규화 방식으로 10+ 레이어 학습 가능
- 깊다고 무조건 더 잘 학습되는 것이 아님(단순히 깊게 하면 오히려 성능이 떨어짐)
  -> **최소한 작은 레이어 수준의 성능은 보장**(잔차블록의 핵심 아이디어)

#### Residual Block(잔차블록)

- 입력 X가 두 경러로 나뉘어 전달됨
  1. 변환 경로(원래 있던 일반적인 경로)(x->F(x))
  2. Shortcut 경로(입력 X 그대로 전달)
- 마지막에 두 값을 더함 : $output=F(x)+x$
- 학습이 잘 안 된다면 F(x)은 0과 비슷한 값이 될 수 있고 출력은 x가 됨
  -> **최소 성능 보장**

#### 💡 “최소 성능 보장”의 의미

만약 학습이 잘 안 되어서 𝐹(𝑥)가 거의 0에 가까워진다면,
**출력:**

$$
output=F(x)+x≈0+x=x
$$

-> 즉, 입력값이 그대로 다음 층으로 전달
이건 마치 그 블록이 아무 일도 하지 않은 것과 같은 상태(identity mapping)가 됨

#### ✅ 이게 왜 좋은가?

이 구조 덕분에 다음과 같은 이점이 생김 👇

1. 망가진 학습 방지 (Degradation 문제 해결)
   깊은 네트워크일수록 학습이 잘 안 되면서 성능이 오히려 나빠지는 경우가 있음
   하지만 Residual Block은 “적어도 입력을 그대로 전달할 수 있으니까”,
   학습이 완전히 실패해도 성능이 나빠지지 않고 최소한 기존 수준(x 그대로의 결과) 은 유지됨

2. 그라디언트 소실 완화 (Gradient Vanishing 완화)
   역전파 시, shortcut 경로 덕분에 gradient가 직접 입력층으로 흘러갈 수 있음
   따라서 𝐹(𝑥)가 잘 학습되지 않아도 네트워크 전체의 학습이 멈추지 않음

#### 일반 잔차 블록 vs 보틀넥 잔차 블록

> **연산 효율**을 위해 **보틀넥 잔차구조**를 도입 :
> 채널 사이즈 축소 후 연산 다시 입출력 해상도 맞추기 -> 연산 효율 높아짐!

#### Stem 구조

- 기존 모델의 효율성 레시피를 잘 활용
  1. 스템 구조를 모델 초기에 도입, 입구 데이터 해상도를 $\frac{1}{4}$ 로 줄이고 이후 잔차 블록 적용
  2. 여러 개의 FC 레이어를 제거하고(GoogleLeNet에서 제안), 전역 평균 풀링을 사용 -> 마지막 1개 FC만 적용

---

### 2-4. MobileNet

- 목표 : 모바일/임베디드 환경에서 구동 가능
  - 기존 합성곱은 공간(HxW)와 채널(C)를 동시 처리하여 연산량이 과도하게 요구됨
- MobileNet 핵심 아이디어 : 공간과 채널을 두 단계로 분리하여 처리
  1. 깊이별 합성곱 : 각 채널별 독립적 3x3 합성곱 수행
  2. 화소별 합성곡 : 1x1 합성곱을 채널방향으로 적용
- 효과
  - **연산량** 기존 대비 9배 가량 **절감**
  - 모델 상수 대폭 감소

---
