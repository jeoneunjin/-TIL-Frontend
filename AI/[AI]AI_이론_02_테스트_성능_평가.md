# 🤖 AI & 기계 학습 기초 (2)

## 📚 목차

1. [테스트 성능 평가](#1-테스트-성능-평가)
2. [검증셋 접근](#2-검증셋-접근)
3. [K-겹 교차검증](#3-k-겹-교차검증)
4. [비지도 학습](#4-비지도-학습)
5. [클러스터링(Clustering)](#5-클러스터링clustering)
6. [계층적 군집(Hierarchical Clustering)](#6-계층적-군집)
7. [클러스터링 시 주의점](#7-클러스터링-시-주의점)

---

## 1. 테스트 성능 평가

### 🔍 훈련 오류 vs 테스트 오류

| 항목           | 설명                                                            |
| -------------- | --------------------------------------------------------------- |
| 🧪 훈련 오류   | 모델이 학습된 **같은 데이터**로 평가한 오류                     |
| 📊 테스트 오류 | **보지 않은 새로운 데이터**(테스트셋)에서 평가한 평균 예측 오류 |

- 일반적으로 **훈련 오류는 테스트 오류를 과소평가**함  
  → 즉, 실제 성능보다 잘 나올 수 있음

### ❗ 왜 중요한가?

> **모델 일반화 성능**은 테스트 오류로 판단해야 하며,  
> 훈련 오류는 이에 대한 잘못된 낙관적 추정이 될 수 있음.

---

## 2. 검증셋 접근

### 2-1. 검증셋(Validation Set) 방법

- 데이터를 무작위로 **훈련셋**과 **검증셋**으로 분할
- 훈련셋으로 모델 학습 → 검증셋으로 성능 평가(MSE, 오분류율 등)

#### 📌 절차

1. 데이터 셔플링 → 훈련/검증으로 분할
2. 훈련셋으로 학습
3. 검증셋으로 테스트 오류 추정

> 📌 일반적으로 70:30 또는 80:20으로 나누지만, 반드시 절반일 필요는 없음

#### ⚠️ 단점

| 문제           | 설명                                                                |
| -------------- | ------------------------------------------------------------------- |
| 분산 큼        | 데이터 분할에 따라 결과가 많이 달라질 수 있음                       |
| 데이터 낭비    | 전체 데이터를 활용하지 못함 (일부만 훈련에 사용됨)                  |
| 과대 추정 경향 | 훈련 데이터가 줄어들어 모델이 충분히 학습되지 않음 → 오류 과대 추정 |

---

## 3. K-겹 교차검증

### 3-1. K-겹 교차검증

- 데이터를 **K개 부분집합(fold)**으로 나눈 뒤,
- 각 fold가 **한 번씩 검증셋이 되고**, 나머지 K-1개는 훈련셋이 됨
- K번 반복 → **평균 오류**를 계산

#### 💡 예: 5-겹 교차검증

- 총 데이터 100개 → 각 fold에 20개씩
- 5번 학습-평가 반복 → 5개의 오류 평균

---

### 3-2. K-겹 교차검증 오류 계산

- 각 fold에서의 **검증 오류(MSE, 오분류율 등)**를 평균하여 최종 오류 추정

> ✅ 데이터 낭비가 없음 (모든 샘플이 최소 한 번은 검증셋이 됨)

---

### 3-3. Leave-One-Out 교차검증 (LOOCV)

- K = n (데이터 수)
- 각 반복마다 **하나의 샘플**을 검증셋으로, 나머지를 훈련셋으로 사용
- 총 n번 반복 → 평균 오류 계산

| 항목   | 내용                          |
| ------ | ----------------------------- |
| 훈련셋 | n-1개 샘플                    |
| 검증셋 | 1개 샘플                      |
| 장점   | 데이터 최대한 활용, 편향 적음 |
| 단점   | 계산 비용 큼, 분산 큼         |

---

### 3-4. K-겹 교차검증 비교

| 항목           | LOOCV   | K-겹 (ex. K=10)                 |
| -------------- | ------- | ------------------------------- |
| 반복 횟수      | n회     | K회                             |
| 검증셋 크기    | 1       | n/K                             |
| 편향(Bias)     | 낮음    | 약간 높음 (적절한 K 선택 시 OK) |
| 분산(Variance) | 높음    | 낮음                            |
| 계산 비용      | 매우 큼 | 상대적으로 적음                 |

#### ✅ 결론

- 대부분의 경우 **10-겹 교차검증이면 충분**
- LOOCV는 **데이터가 매우 적을 때** 또는 **이론적 분석 시** 사용

---

## 💡 추가 팁: 데이터 분할 전략

| 전략          | 특징                                   | 사용 시기                 |
| ------------- | -------------------------------------- | ------------------------- |
| Hold-Out      | 한 번만 분할 (훈련 vs 검증)            | 빠르게 성능 확인할 때     |
| K-겹 교차검증 | K번 분할/평가 → 평균                   | 일반적인 모델 평가에 적합 |
| LOOCV         | 가장 극단적인 형태, 데이터 최대한 활용 | 데이터 수가 매우 적을 때  |

---

## 4. 비지도 학습

> 레이블(정답) 없이 데이터의 구조·패턴·집단(잠재 서브그룹)을 찾아내는 학습

- 대표 과제: 군집화(Clustering), 차원 축소(PCA 등), 밀도 추정 / 이상치 탐지
- 출력: 정답 예측이 아닌 **데이터 구조의 요약·표현(embedding)**

- 핵심 질문
  - 무엇을 **비슷함 / 다름**으로 볼 것인가? → 거리·유사도 선택
  - 어떤 방법으로 **묶을 것 / 표현할 것인가?**

---

## 5. 클러스터링(Clustering)

> 데이터 안에서 **하위 집단(클러스터)** 을 찾는 기법들의 총칭

- 목표:  
  클러스터 내부는 서로 유사, 클러스터 간은 상이하도록 데이터를 분할
- 유사/상이 정도는 도메인 맥락에 따라 정의가 달라질 수 있음
- 문제: 데이터 특성에 의존

- 예시: **마케팅 세그먼테이션**

  > 여러 지표를 가진 고객 데이터를 기반으로 특정 상품·광고에 반응할 하위 집단을 식별하는 것  
  > → 시장 세분화 작업 자체가 클러스터링에 해당

- 두 가지 대표 클러스터링 기법
  1. **K-평균(K-means)** : K(클러스터 수)를 미리 정해 분할
  2. **계층적 군집(Hierarchical)** : K를 사전에 고정하지 않음

---

## 5-1. K-means 클러스터링

> 클러스터 색상은 의미 없고, 다른 색 = 서로 다른 클러스터 소속

- 각 관측치는 적어도 하나의 클러스터에 속함
- 모든 클러스터는 서로 **겹치지 않음(교집합 = ∅)**

- 핵심 아이디어 💡

  - **좋은 군집화** = 클러스터 내부 변동이 작은 분할
  - 목표: **클러스터 내부 변동의 합을 최소화**하는 분할 찾기

- K-means는 **거리 기반 알고리즘**이므로  
  변수의 단위가 다를 경우 **표준화(Standardization)** 필수!

---

## 5-2. K-means 알고리즘

1. **초기화:** 관측치들에 무작위로 1...K 클러스터를 임시 부여
2. **반복(할당이 더 이상 바뀌지 않을 때까지):**
   - 2a. 각 클러스터의 중심(centroid) 계산 (특성 평균 벡터)
   - 2b. 각 관측치를 **가장 가까운 중심(유클리드 거리 등)** 의 클러스터로 재할당
3. **수렴:** 더 이상 클러스터 변경이 없을 때 종료

- 알고리즘 특성:
  - 각 반복 단계는 **군집 내 제곱 거리 합을 감소시킴**
  - 단, **전역 최솟값 보장은 아님** (→ 초기값에 따라 지역 최솟값 가능)
  - ⇒ **여러 번 시도 후 가장 좋은 결과 선택 권장**

---

## 5-3. K-means 클러스터링에서 초기값의 영향

- **서로 다른 초기 레이블** → 최종 분할 결과와 목표값이 달라짐
- 초기화의 중요성:
  - 무작위 초기화 시 결과 불안정 가능
  - **K-means++** 초기화: 중심을 더 균등하게 선택하여 수렴 안정화
  - 여러 번 반복(`n_init` 파라미터 사용) 후 최소 SSE를 가지는 결과 선택

---

## 6. 계층적 군집

---

## 6-1. 계층적 군집(Hierarchical Clustering)

> **K-means vs 계층적 군집**

| 구분          | K-means                   | 계층적 군집               |
| ------------- | ------------------------- | ------------------------- |
| 클러스터 수   | K 사전 지정 필요          | K 미지정, 자동 탐색 가능  |
| 방식          | 분할형(Partitioning)      | 병합형(Agglomerative)     |
| 시각화        | X                         | O (덴드로그램 제공)       |
| 계산량        | 낮음 (대규모 데이터 적합) | 높음 (소규모 데이터 적합) |
| 초기값 민감도 | 높음                      | 없음                      |

- 계층적 군집은 전체 데이터의 구조를 **덴드로그램(Dendrogram)** 으로 표현
- 덴드로그램의 **수평선 높이(거리)** 를 기준으로 “가위질”하여 K개의 군집 결정

---

## 6-2. 계층적 군집 알고리즘(상향식)

1. n개의 관측치를 각각 **하나의 클러스터로 시작**
2. 모든 클러스터 쌍의 **비유사도(거리)** 계산
3. 가장 유사한 두 클러스터를 **병합**
4. 병합 후 남은 클러스터 간 거리 재계산
5. 클러스터 개수가 1개가 될 때까지 반복

> 결과적으로 단계별 병합을 나타내는 **덴드로그램 생성**

---

## 6-3. 계층적 군집 단계별 진행

- 병합 과정에서 데이터가 점차 큰 클러스터로 합쳐짐
- 1개의 단일 클러스터가 될 때까지 반복
- 이후 덴드로그램에서 적절한 “높이”에서 잘라 K개의 클러스터 선택

📉 **계산량:**  
매 단계마다 모든 클러스터 쌍의 거리를 계산해야 하므로  
데이터 수가 많을 경우 **K-means보다 계산 부담이 큼**

### 클러스터 간 거리 계산 방법(링키지 방식)

- **단일 연결법 (Single linkage)**: 두 클러스터 중 **가장 가까운 점** 거리
- **완전 연결법 (Complete linkage)**: 두 클러스터 중 **가장 먼 점** 거리
- **평균 연결법 (Average linkage)**: 두 클러스터 간 모든 점 거리의 평균
- **워드 연결법 (Ward’s method)**: 클러스터 내 제곱합 증가량 최소화 (K-means와 유사)

### 핵심 요약

| 항목          | K-means               | 계층적 군집                |
| ------------- | --------------------- | -------------------------- |
| K값 필요 여부 | O (사전 지정)         | X (자동 구조 탐색 후 선택) |
| 결과 형태     | K개의 고정된 클러스터 | 덴드로그램(계층 구조)      |
| 초기값 영향   | 큼                    | 없음                       |
| 계산 효율     | 빠름                  | 느림                       |
| 시각화        | X                     | O                          |
| 대규모 데이터 | 적합                  | 비적합                     |

---

## 6-4. 계층적 군집의 링크 유형

1. single(최소 거리)링크 : 두 개 클러스터 내 데이터 쌍별 거리 중 최소값을 군집 간 거리로
2. Complete(최대 거리)링크 : 두 개 클러스터 내 데이터 쌍별 거리 중 최대값을 군집 간 거리로
3. Aberage(평균 거리)링크 : 두 개 클러스터 내 데이터 쌍별 거리의 평균을 군집 간 거리로

> **링크에 따라 결과가 달라진다!**
>
> - 같은 데이터라도 링크 선택에 따라 클러스터링 결과(덴드로그램)가 달라질 수 있음
> - 하나의 링크만 시도하는 것이 아니라 다른 종류의 링크도 사용함을 권장

---

## 7. 클러스터링 시 주의점

1. 스케일링 : 표준화(평균 0, 표준편차 1로 입력 변수 변환)가 필요!

   > 변수 단위 차이 영향이 큼

2. 몇 개의 클러스터가 적합한지?
   > K-means, 계층적 모두 어려운 문제. 합의된 정답은 없음

**단일 시도가 아닌 여러 번 시도 권장**

---
