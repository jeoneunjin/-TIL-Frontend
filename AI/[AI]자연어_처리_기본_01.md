# 🤖자연어 처리 기본(1)

## 📚 목차

- ***

---

## 1.워드 임베딩

### 1-1. 원-핫 인코딩

#### 원 핫 인코딩의 문제점

- 검색 쿼리 벡터와 대상이 되는 문서 벡터들이 서로 직교하게 되어, 원 핫 벡터로는 유사도를 측정할 수 없음

- 전통적인 텍스트 표현 방식에 여러 한계가 존재함

1. 차원의 저주
2. 의미적 정보 부족

### 1-2. 워드 임베딩

- 주변 단어들을 통해서 특정 단어의 의미를 파악함
  ex. "정부는 가계부채 문제를 해결하기 위해 **_은행_** 대출 규제를 강화했다."
  -> **_은행_** 주변 단어들로 **은행**의 의미를 나타낼 수 있음

- **워드 임베딩이란?**
  > - 단어들 단어들 사이의 의미적 관계를 포착할 수 있는 밀집되고 연속적/분산적 벡터 표현으로 나타내는 방법
  - 대표적인 워드 임베딩 기법 - **Word2Vec**

#### Word2Vec의 두 가지 알고리즘

1. Skip-grams(SG) 방식
2. Continuous Bag of Words(CBOW) 방식

#### 1. SG 방식

> 윈도우 크기 설정해서 **중심 단어**의 **주변 단어들**(윈도우 크기 내에 있는 단어들)를 통해서 **의미 추측**하는 알고리즘

- 윈도우 크기 = 중심 단어 주변 몇 개 단어를 문맥으로 볼 것인가?

#### 2. CBOW 방식

> **주변 단어들의 집합**을 통해 함께 등장할 수 있는 **단일 단어 예측**하는 알고리즘

- !예측해서 나온 값이 실제 값과 다를 수 있음!

#### SG vs CBOW

| 모델      | 장점                                                | 한계                    |
| --------- | --------------------------------------------------- | ----------------------- |
| skip-gram | 적은 데이터에도 잘 동작, 희귀 단어나 구 표현에 강함 | 학습 속도가 느림        |
| CBOW      | 학습 속도가 빠르다, 자주 나오는 단어에 강하다       | 희귀 단어 표현에 약하다 |

=> 희귀 단어가 생각보다 많다. 보통 skip-gram을 사용함

---

## 2. 순차적 데이터

### 2-1. 순차적 데이터란?

- 자연에 수많은 순차적 데이터가 존재함
- 데이터가 입력되는 순서와 이 순서를 통해 입력되는 데이터들 사이의 관계가 중요한 데이터

### 2-2. 순차적 데이터 특징

1. 순서가 중요
2. 장기 의존성
3. 가변 길이

---

## 3. RNN

### 3-1. 전통적인 인공신경망 vs RNN

> 전통적인 인공신경망(MLP, CNN)들은 고정된 길이의 입력을 받아 가변 길이의 데이터를 처리하기에 적합하지 않음
> 반면, RNN은 가변 길이의 입력을 받을 수 있고, 이전 입력을 기억할 수 있기 때문에, 순차적 데이터 처리에 적합한 아키텍쳐

### 3-2. 특징

- 이전 시점의 정보를 담는 `hidden state`를 가지고 있음
  - 입력 시퀀스 벡터 $\x$를 처리할 때, 각 시점마다 `recurrence` 수식을 적용하여 `hidden state`를 업데이트
  - `recurrence` 수식
    $$
    h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t)
    $$
- **한 번에 하나의 요소를 처리**하고, 정보를 **앞으로** 전달
- 펼쳐서 보면, RNN은 각 층이 하나의 시점을 나타내는 깊은 신경망처럼 보임
- `hidden state`를 유지하면서 **가변 길이 데이터**를 처리할 수 있음
- RNN의 출력은 **과거 입력**에 영향을 받는다는 점에서 `feedforward` 신경망과 다름

### 3-3. 한계

- 기울기 소실(vanishing gradient) 문제

---

## 4. LSTM

### 4-1. LSTMs란?

- 기울기 소실 문제를 해결하기 위해, 제안된 RNN의 한 종류

### 4-2. 특징

- 시점 $t$ 에서 RNN은 길이가 $n$인 벡터 hidden state $h_t$ 와 cell state $C_t$를 가짐
  - Hidden state는 short-term information 저장
  - Cell state는 long-term information 저장
- cell state에서 정보를 읽고, 지우고, 기록할 수 있음

### 4-3. RNNs vs LSTMs

### 4-4. 3가지 게이트

- Forgate gate : 이전 cell state에서 무엇을 버리고 무엇을 유지할지 결정
- Input gate : 새 정보 중 얼마나 cell state에 쓸지 결정
- Output gate : cell state 중 얼마나 hidden state로 내보낼지 결정

#### 게이트 동작

- 매 시점마다 게이트의 각 요소는 열림(1), 닫힘(0) 혹은 그 사이의 값으로 설정됨
- 게이트는 동적으로 계산되며, 현재 입력과 hidden state 등 문맥에 따라 값이 정해짐

---

## 5. 자연어 생성 모델

### 5-1. 언어모델이란?

> 인간의 두뇌가 자연어를 생성하는 능력을 모방한 모델

- 단어 시퀀스 전체에 **확률**을 부여하여 문장의 **자연스러움을 측정**함
- 한 문장의 확률은 각 단어의 **\*조건부 확률들의 곱**으로 표현 가능

### 5-2. 대표적인 언어모델 - N-gram 언어모델

> n-gram이란, 연속된 n개의 단어 묶음을 말함

- 다양한 **n-gram**이 얼마나 자주 등장하는지 통계를 수집하고(**발생 빈도 수집**), 이를 활용해 다음 단어를 예측

- n-gram 모델은 예측한 단어를 이전 단어들과 이어 붙이며 문장을 확장
  - 하지만 4-gram 이상이 되면 조합의 등장 확률이 매우 낮아져 통계적으로 의미 있는 예측이 어렵기 때문에 보통 4-gram까지만 사용

## 6. Seq2Seq

### 6-1. Neural Machine Translation이란?

> 인공 신경망을 이용해 기계 번역을 수행하는 방법;

- 이때 사용되는 신경망 구조 **sequence-to-Sequence**; 두 개의 RNNs으로 이루어짐

### 6-2. Seq2Seq 등장 배경

- 번역 문제는 입력과 출력의 길이가 다를 수 있다.
  -> NMT에서는 길이가 다른 시퀀스 간의 매핑을 처리할 수 있어야 함
  -> 따라서, 2개의 LSTM을 이용하자! - 한 LSTM은 Encoder, 다른 LSTM은 Decoder의 역할을 함

### 6-3. SeqSeq 아키텍처

- **Encoder**와 **Decoder**로 이루어짐

  > Encoder : 입력 문장에 담긴 정보를 인코딩
  > Decoder : 인코딩된 정보를 조건으로 하여 타겟 문장(출력)을 생성

- 인코더와 디코더가 **하나의 통합 네트워크**로 연결되어 있음
  > 디코더에서 발생한 오차는 역전차 과정을 통해 입력을 처리한 인코더까지 전달되어 전체 네트워크가 End-to-End-로 동시에 최적화됨

### 6-4. 다양한 적용

> 기계번역 외에도 다양한 태스크에 적용할 수 있음

1. 요약
2. 대화
3. 코드 생성

### 6-5. 학습 수행

- **Teacher Forcing**
  > 모델이 스스로 예측한 단어 대신 정답 단어를 디코더 입력으로 강제로 넣어줌으로써 훨씬 안정적이고 빠르게 학습을 수행하는 방법

### 6-6. 토큰 출력 방법

1. Greedy Inference

- 토큰을 출력하는 방법 중 하나로, 각 단계에서 가장 확률이 높은 단어를 선택

- 한계 :
  - 되돌리기가 불가능! = 오답이어도 되돌아갈 수 없음

2. Beam Search

- 각 스텝에서 탐색의 영역을 k개의 가장 가능도가 높은 토큰들로 유지하며 다음 단계를 탐색(이때 k는 사용자가 지정하는 hyper-parameter)

- 진행 과정 :
  1. 매 단계마다 k개의 가장 유망한 후보 유지
  2. 후보가 <\EOS>에 도달하면, 완성된 문장으로 리스트 추가
  3. <\EOS> 문장이 충분히 모이면 탐색 종료
  4. 각 후보들의 점수를 로그 확률의 합으로 구해 최종 선택

---

## 7. Attention

### 7-1 Attention

#### RNN 한계

- Bootleneck problem - 인코더는 입력 문장 전체를 하나의 벡터로 요약하는데, 마지막 hidden state에 문장의 모든 의미 정보가 담김
- 고정 길이 벡터 하나에 모든 문장의 의미를 압출하다 보니 정보 손실이 생길 수 있음

-> Attention이 이를 보완

### 7-2 Attention 효과

1. NMT 성능향상
2. Bottleneck Problem 해결
3. Vanishing Gradient Problem 완화

- 해석 가능성
- 정렬

### 7-3 Query와 Values

---

## 8. Transformer

#### RNN 한계

1. 장기 의존성
2. 병렬화

-> Self-Attention이 이를 보완

#### 장점

1. 순차적으로 처리해야 하는 연산 수가 시퀀스 길이에 따라 증가하지 않음
2. 최대 상호작용 거리 = O(1) -> 묻는 단어가 각 층에서 직접 상호작용

### 8-3 Key, Query, Value

1.  각 단어 "$i$"를 표현하는 Query와 Value 벡터가 있었듯이, Self-Attention에서는 각 단어 "$i$"를 표현 하는 Query, Key, Value 벡터가 존재

    > ! 3개로 나누는게 필수적인 건 아님

        - Query :
        단어 "$i$"가 다른 단어로부터 어떤 정보를 찾을지를 정의하는 벡터
        - Key :
        단어 "$i$"가 자신이 가진 정보의 특성을 표현하는 벡터
        - Value :
        실제로 참조되는 정보 내용을 담고 있는 벡터

2.  Query, Keys 간의 유사도를 계산해, softmax로 확률분포를 구함
3.  각 단어의 출력을 Values의 가중합으로 계산

### 8-2. 한계

1. 순서 정보 부재
2. 비선형성 부족
3. 미래 참조 문제
