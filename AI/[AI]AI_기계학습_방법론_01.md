# 🤖 AI & 기계학습 방법론(1)

## 📚 목차

1. [선형회귀(Linear Regression)](#1-선형회귀linear-regression)
2. [단순선형회귀(Simple Linear Regression)](#2-단순선형회귀simple-linear-regression)
3. [다중선형회귀(Multiple Linear Regression)](#3-다중선형회귀multiple-linear-regression)
4. [선형회귀 주의사항](#4-선형회귀-주의사항)
5. [분류(Classification)](#1-분류classification)
6. [로지스틱(Logistic) 회귀](#2-로지스틱logistic-회귀)

---

## 1. 선형회귀(Linear Regression)

> 입력 변수와 출력 변수 사이의 관계를 직선 형태로 근사하여 예측하는 통계적 방법

- 선형회귀를 통해 알 수 있는 예시:
  - 광고비와 매출 사이에 관계가 있는가?
  - 그 관계의 강도는 어느 정도?
  - 어떤 매체가 매출에 기여하는가?

---

## 2. 단순선형회귀(Simple Linear Regression)

### 2-1. 단일 설명변수를 이용한 선형회귀

> 한 개의 **설명변수 \(X\)**와 하나의 **반응변수 \(Y\)** 사이의 선형 관계

- 목표 : 데이터를 가장 잘 설명하는 직선을 찾아 예측
- 모형 가정 :

$$
Y = \beta_0 + \beta_1 X_1 + \epsilon
$$

- $\beta_0$ : 절편 (X=0일 때 Y 값)
- $\beta_1$ : 기울기 (X가 1 단위 증가할 때 Y 평균 증가량)
- $\epsilon$ : 관측 오차

### 2-2. 최소제곱법(least squares)

- RSS (Residual Sum of Squares):

$$
RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

- 목표 : RSS 최소화 → $\beta_0$, $\beta_1$ 추정
- Closed-form solution 가능

---

## 3. 다중선형회귀(Multiple Linear Regression)

### 3-1. 다중선형회귀란?

- 독립 변수 $X_1, X_2, ..., X_p$가 여러 개 존재할 때 사용하는 회귀 분석 기법
- 목표: 데이터와 가장 가까운 평면(hyperplane) 찾기
- 모형 가정 :

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon
$$

- 회귀 계수:

$$
\beta_1, \beta_2, ..., \beta_p
$$

: 각 독립 변수의 영향력과 방향

### 3-2. 다중선형회귀의 추정과 예측

- 여러 입력 변수를 동시에 고려하여 데이터와 가장 가까운 평면(hyperplane) 찾음
- 최소제곱법(RSS 최소화) 사용

### 3-3. 다중선형회귀 계수 추정 (행렬 표현)

$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$

---

## 4. 선형회귀 주의사항

| 항목                | 설명                                                        |
| ------------------- | ----------------------------------------------------------- |
| 검증/테스트 데이터  | 학습 데이터 성능만으로 판단하지 않고, 일반화 성능 확인 필요 |
| 다중공선성          | 변수 간 상관이 높으면 계수 추정이 불안정, 해석 혼동 가능    |
| 상관관계와 인과관계 | 관찰 데이터의 상관만으로 인과 주장 불가                     |

---

## 5. 분류(Classification)

- 목표 : 입력 X가 속할 **범주(카테고리)** 예측
- 범주형 변수: 순서 없음 (예: 성별, 혈액형, 지역)
- 선형회귀는 분류 문제에 부적합 ❌
  - 예측값이 0~1 범위 벗어날 수 있음
  - 다중 범주 문제 → 잘못된 해석

### 5-1. 적합한 모델: 로지스틱 회귀

- 시그모이드 함수 사용 → 0~1 확률값 출력
- 순서 없는 범주 예측에 적합

---

## 6. 로지스틱(Logistic) 회귀

### 6-1. 시그모이드 함수

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

- $e$: 오일러 수 (약 2.718)
- $0 \le \sigma(z) \le 1$
- 극한값:
  - $z \to +\infty \implies \sigma(z) \to 1$
  - $z \to -\infty \implies \sigma(z) \to 0$

### 6-2. 로지스틱 회귀 모형식

$$
p(Y=1|X) = \sigma(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)
$$

- 결과값 항상 0~1 → 확률 예측에 적합
- logit 변환 시 선형 회귀 모형으로 해석 가능

### 6-3. 오즈(Odds)와 로짓(Logit)

$$
\text{odds} = \frac{p}{1-p}, \quad
\text{logit}(p) = \log\frac{p}{1-p} = \beta_0 + \beta_1 X
$$

### 6-4. MLE(Maximum Likelihood Estimation)

- 우도(Likelihood) 최대화 → 베타 추정
- 로그 변환 → log-likelihood → 수치적 최적화

#### ❓ 로그 변환 가능?

> ✅ **Yes!** log 함수는 **단조(monotone) 증가** 함수이므로 최적값 유지

---

## 📌 정리 요약

| 구분          | 특징            | 수식                                                       |
| ------------- | --------------- | ---------------------------------------------------------- | -------------------------------------------- |
| 단순선형회귀  | 하나의 설명변수 | $Y = \beta_0 + \beta_1 X_1 + \epsilon$                     |
| 다중선형회귀  | 여러 설명변수   | $Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon$ |
| 로지스틱 회귀 | 분류 문제       | $p(Y=1                                                     | X) = \frac{1}{1+e^{-(\beta_0 + \beta_1 X)}}$ |

✅ 주의사항:

- 상관관계 ≠ 인과관계
- 회귀계수 해석 시 다중공선성 주의
- 분류 문제에 선형회귀 대신 로지스틱 회귀 사용
