# 🤖 AI & 기계학습 방법론(1)

## 📚 목차

1. [선형회귀(Linear Regression)](#1-선형회귀linear-regression)
2. [단순선형회귀(Simple Linear Regression)](#2-단순선형회귀simple-linear-regression)
3. [다중선형회귀(Multiple Linear Regression)](#3-다중선형회귀multiple-linear-regression)
4. [선형회귀 주의사항](#4-선형회귀-주의사항)
5. [분류(Classification)](#1-분류classification)
6. [로지스틱(Logistic) 회귀](#2-로지스틱logistic-회귀)

---

## 1. 선형회귀(Linear Regression)

> 입력 변수와 출력 변수 사이의 관계를 직선 형태로 근사하여 예측하는 통계적 방법

- 선형회귀를 통해 알 수 있는 예시:
  - 광고비와 매출 사이에 관계가 있는가?
  - 그 관계의 강도는 어느 정도?
  - 어떤 매체가 매출에 기여하는가?

---

## 2. 단순선형회귀(Simple Linear Regression)

## 2-1. 단일 설명변수를 이용한 선형회귀

> 한 개의 **설명변수 \(X\)**와 하나의 **반응변수 \(Y\)** 사이의 선형 관계

- 목표 : 데이터를 가장 잘 설명하는 직선을 찾아 예측

- 모형 가정 :

$$
Y = \beta_0 + \beta_1 X_1 + \epsilon
$$

> $\beta_0$ : 절편(X=0일 때의 Y 값)
>
> $\beta_1$ : 기울기(X가 1단위 증가할 때 Y의 평균 증가량)
>
> $\epsilon$ : 관측 오차

## 2-2. 최소제곱법(least squares)

> **실제 관측값과 예측값의 차이**(잔차, residual)를 제곱해 합한 값(RSS, **잔차제곱합**)을 최소화하는 방법

- RSS :
  \[
  RSS = \sum\_{i=1}^{n} (y_i - \hat{y}\_i)^2
  \]
- 목표 : 데이터를 가장 잘 설명하는(=RSS를 최소화하는) 직선을 찾음; $\beta_0$, $\beta_1$을 추정
- **Closed-form solution** 가능
  > 공식으로 바로 계산할 수 있는 해가 존재함!

## 2-3. 광고 데이터

## 3. 다중선형회귀(multiple linear regression)

## 3-1. 다중선형회귀란?

> 독립 변수 \(X_1, X_2, ..., X_p\)가 여러 개 존재할 때 사용하는 회귀 분석 기법
> 여러 독립 변수를 동시에 고려하여, 종속변수 $\Y$와의 관계를 구함

- 목표: 데이터와 가장 가까운 평면(hyperplane) 찾기

- 모형 가정 :

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon
$$

- \(\beta_1, \beta_2, ..., \beta_p\): 각 독립 변수에 대한 회귀 계수(모수) = 번수의 영향력 크기와 뱡향을 나타냄

## 3-2. 다중선형회귀의 추정과 예측

- 단위선형회귀와 마찬가지로 최소제곱법으로 RSS를 최소화

- 여러 입력 변수를 동시에 고려하여 데이터와 가장 가까운 평면(hyperplane)을 찾는 과정

## 3-3. 다중선형회귀 계수 추정 유도(행렬 표현)

\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]

## 3-4. 다중선형회귀 시각화

## 3-5. 다중선형회귀 결과: 광고데이터

---

## 4. 선형회귀 주의사항

| 항목                | 설명                                                        |
| ------------------- | ----------------------------------------------------------- |
| 검증/테스트 데이터  | 학습 데이터 성능만으로 판단하지 않고, 일반화 성능 확인 필요 |
| 다중공선성          | 변수 간 상관이 높으면 계수 추정이 불안정, 해석 혼동 가능    |
| 상관관계와 인과관계 | 관찰 데이터의 상관만으로 인과 주장 불가                     |

---

## 5. 분류(Classification)

## 5-1. 분류란?

> 데이터가 속할 **범주(카테고리)**를 예측하는 문제

- 범주형 변수: 순서 없음 (예: 성별, 혈액형, 지역)
- 목표: 분류 함수 \(f(X)\) 학습 → 입력 X가 속할 범주 예측

## 5-2. 예시 : 신용카드 연체(Default)

- 신용카드 사용량 및 소득에 대한 연체 여부 산점도

  - 신용카드 사용량(Balance) - 소득(Income) 산점도에 연체(Default) 여부를 색상(주황색 vs 파란색) 및 부호 (+ vs O)로 구분
  - 신용카드 사용량과 소득은 각각 독립변수, 연체 여부가 종속 변수

- 신용카드 사용량-소득 산점도 해석
  - 연체자(주황색 +)는 신용카드 사용량이 높은 구간에 집중적으로 분포
  - 연체가 없는 사람(파랑색 O)은 신용카드 사용량이 낮은 쪽에 주로 분포
  - 소득은 연체 여부와 뚜렷한 상관이 보이지 X
- 그룹별 분포(Boxplot)
  - 신용카드 사용량, 소득에 대해 연체의 그룹별 분포
- 그룹별 분포 해석

  - 신용카드 사용량 vs 연체

    > 연체자의 신용카드 사용량이 연체하지 않은 사람버디 전반적으로 높음
    > 중앙값도 높고, 분포가 퍼져 있는 정도도 더 큼

    - 소득 vs 연체
      > 연체 여부에 따른 소득 차이는 거의 없음

## 5-3. 분류 문제에 선형회귀를 써도 될까?

> 선형회귀는 분류 문제에 사용하기에 부적절함

❌ 부적절

- 이진 분류 문제: 예측값이 0~1 범위 안에 있지 않음
- 다중 범주 문제: 범주 간 순서 가정 → 잘못된 해석

## 5-4. 분류 문제에서 적합한 모델

- **로지스틱 회귀**
  - 시그모이드 함수 사용 → 0~1 확률값 출력
  - 순서 없는 범주를 확률로 직접 예측

---

## 6. 로지스틱(Logistic) 회귀

## 6-1. 로지스틱 회귀의 모형식

### 시그모이드 함수와 로지스틱 회귀

- **시그모이드 함수(Sigmoid Function)**  
  S자 형태의 곡선 함수로, 모든 실수 입력을 0~1 범위의 값으로 변환

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

> - \(e\) : 오일러 수 (약 2.718)
> - 모든 실수 \(z\)에 대해 \(0 \leq \sigma(z) \leq 1\)
> - 극한값:
>   - \(z \to +\infty \implies \sigma(z) \to 1\)
>   - \(z \to -\infty \implies \sigma(z) \to 0\)

- **로지스틱 회귀 모형식**  
  선형 회귀식 \(z = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p\)를 시그모이드 함수에 대입하여 확률값 출력

\[
p(Y=1|X) = \sigma(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)
\]

> - 결과값은 항상 0~1 사이 → 확률 예측에 적합
> - z 값에 따라 성공 확률이 변하며, logit 변환 시 선형 모형으로 해석 가능

## 6-2. 오즈(Odds)와 로짓(Logit)

- 오즈(Odds)

  > 성공(`y=1`)확률이 실패(`y=0`)확률에 비해 몇 배 더 높은가를 나타냄
  > `oods = 성공확률/실패확률`

- 로짓 변환(logit) = Log odds
  > 로짓변환은 오즈(odds)에 `log`를 취한 함수 상태

\[
\text{odds} = \frac{p}{1-p}, \quad
\text{logit}(p) = \log\frac{p}{1-p} = \beta_0 + \beta_1 X
\]

=> 즉, 로직스틱 모형식은 선형 모형식과 시그모이드 함수의 결합이며, 로짓 변환시 **선형 회귀 모형식**으로 표현이 가능함

## 6-3. MLE(Maximum Likelihood Estimation) 활용 모수 추정

> 확률을 계산하는 함수를 평가하기 위해선 **우도**를 지표로 삼음

- 우도(Likelihood) :
  **"현재 확률 함수가 데이터를 얼마나 잘 설명하는지"**를 나타낸 지표

- 목적: 우도(Likelihood) 최대화
  > 로지스틱 회귀와 같은 이진 분류 문제에서 우도를 최대화 함
- 로그 변환 → **log-likelihood** 계산
  > 곱으로 이뤄진 함수의 경우 베타에 대해 미분이 어렵기 때문에 log를 취한 후 **log-likelihood**를 만들어 최대화 함
- 수치적 최적화로 베타 추정
  > 로그 변환 후, 미분하여 도함수=0에 근접하도록 수치적 최적화를 통해 베타들을 찾음

#### ❓log로 변형하여 문제를 접근해도 되는 것인가

> ✅ **Yes!** log 함수는 **단조(monotone) 증가 함수**이므로 최적값이 그대로 유지됨

## 6-4. 로지스틱 회귀 결과: 신용카드 연체 데이터

- 입력값 2배 증가 → 예측 확률 97배 증가

---

## 📌 정리 요약

| 구분          | 특징            | 수식                                                         |
| ------------- | --------------- | ------------------------------------------------------------ | --------------------------------------------- |
| 단순선형회귀  | 하나의 설명변수 | \(Y = \beta_0 + \beta_1 X_1 + \epsilon\)                     |
| 다중선형회귀  | 여러 설명변수   | \(Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon\) |
| 로지스틱 회귀 | 분류 문제       | \(p(Y=1                                                      | X) = \frac{1}{1+e^{-(\beta_0 + \beta_1 X)}}\) |

✅ 주의사항:

- 상관관계 ≠ 인과관계
- 회귀계수 해석 시 다중공선성 주의
- 분류 문제에 선형회귀 대신 로지스틱 회귀 사용
