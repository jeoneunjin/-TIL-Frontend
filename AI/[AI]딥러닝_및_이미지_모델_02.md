# 🤖 딥러닝 및 이미지 모델(2)

## 📚 목차

- ***

---

## 1. 학습 전략의 중요성

### 좋은 모델 구조 = 우수한 성능?

- 좋은 구조만으로는 성능 보장 불가!
  > 학습 과정에서 발생하는 불안정성, 과적합, 수렵 속도 문제

#### 학습에서 자주 겪는 문제들

1. 학습 불안정 : 손실 폭발 학습 멈춤 수렴 실패
2. 과적합 : 훈련데이터에는 맞지만 검증, 실전 성능 저하
3. 느린 수렴 : 최적점에 도달하리까지 불필요하게 많은 에폭 소모로 인한 학습 효율 저하

## 2. 활성화 함수

### 2-1. 활성화 함수

> 입력 신호의 총합을 출력 신호로 변환하는 함수

#### 역할

- 신경망에 비선형성 부여 -> 복잡한 패턴 학습 가능
- 활성함수가 없다면 단순 선형모델과 동일! 단, 활성화 함수의 특성에 따라 학습 안정성과 성능이 크게 좌우

### 대표 활성화 함수 - Sigmoid

- 0~1 사이 값으로 출력 값을 제어

#### 문제 1

- sigmoid 함수는 입력이 매우 크거나 매우 작으면 출력이 거의 0이나 1에 고정
  -> 더이상 역전파 전달 X

#### 문제 2

- Sigmoid 출력 범위는 항상 양수
  -> 학습 과정에서 편향된 업데이트 발생
  -> 기울기의 평균이 한쪽으로(양수) 치우쳐 학습 비효율 발생

#### 문제 3

- 계산식에 지수 함수가 들어감
  -> ReLU 처럼 단순한 max 연산보다 계산 비용이 더 큼

### 대표 활성화 함수 - Tanh

### 대표 활성화 함수 : Leaky ReLU

### 대표 활성화 함수 : ELU

### 결론 ?

> ReLU부터 시도해라!

---

### 2-2. 데이터 전처리

#### 데이터 형식 통일

#### 모델별 전처리 방식 예시

---

### 2-3. 모델 상수 초기화(중요!)

#### 아이디어1 - 0으로 초기화

- 모든 가중치(W)와 편향(b)을 0으로 초기화하면 무슨 일이 생길까?
  > 모든 출력은 0, 학습이 이루어지지 않음

#### 아이디어2 - 임의(랜덤) 초기화

- 작은 랜덤 숫자 : Gaussian 분포를 따름, 0을 중심으로

  > 깊지 않은 모델에서 동작하는 전략!

- 깊은 모델에서는 상수가 크면 기울기 폭발

#### 아이디어3 - 자비에 초기화

- 가중치 초기 분포의 분산을 입력 차원으로 맞춤
- 분산을 입력 차원(뉴런수) 증가를 고려하여 설계

  > 모델 사수 분산이 입력 차원과 같다면, 선형연산 이후 출력의 분산=입력의 분삼임을 보일 수 있음

- 자비에 초기화 + ReLU 궁합
  - 자비에에서 입출력 분산을 맞추는 전랴긍ㄹ 입출력이 대칭적 분포를 보인다는 가정 하에서 설계
  - ReLU는 양의 본포만 남김. 필연적으로 비대칭!

#### 아이디어4 - 허 초기화

- 가중치 초기 분포의 분산을 2\*입력 차원으로 맞춤
- 비대칭, 유사 분산이 레이어 후반에도 유지됨

---

### 2-4 모델 정규화

- 학습과정에서 드디어 오차가 준다!
- 그러나 검증 오차는 오히려 오른다면,
  - 과적합 신호
  - 정규화를 통해 오차를 줄여야 함

#### 아이디어1 - 가중치 감소

- 학습 과정에서 특정 가중치가 지나치게 커지는 현상 방지
- 가중치 크기에 비례한 패널티 적용

1. L2 정규화(Ridgge, Weight Decay)

- 작동 방식 : 큰 가중치에 제곱으로 패널티 -> 극단적으로 큰 값 방지
- 결과 : 모든 가중치가 골고루 작아짐

2. L1 정규화(Lasso)

- 작동 방식 : 모든 가중치에 절댓값 크키만큼 동일하게 피널티
- 결과 : 중요하지 않은 가중치는 완전히 0이 됨 (희소한 모델)
- 장점 : 자동으로 중요한 특성만 선택 -. 더 단순한 모델

=> 상수의 희소성(가능하면 보다 작은 모델을 선호함)을 강제하고 싶다면, L1 가중치 감소를 선택

3. Elastic net(L1 + L2)

#### 아이디어2 - 드롭아웃

- 학습 과정에서 일부 뉴런을 확률적으로 끔(0으로 설정)
- 매 학습 스텝마다 다른 네트워크 구조가 샘플링되는 효과

- 장점 :

  - 과적합 방지 -> 네트워크가 특정 패턴에 의존하지 X, 일반화 성능 높아짐
  - 여러 작은 모델을 합친 것과 비슷한 효과 (성능 개선을 위한 앙상블 방식과 유사)
  - 간단하면서도 매우 강력

- 단점 :
  - 학습시가 증가
  - 최적의 드롭 비율을 찾는 것이 주용
  - 모델에 따라 비선호 되기도 함

---

## 3. 학습 안정성 전략

### 3-1. 학습 비율 조정

#### 학습률 계단식 변경

> **학습률 매우 중요하다!**

### 3-2. 하이파라미터 선정
