# 🤖 딥러닝 및 이미지 모델(2) 정리

## 📚 목차
- [1. 학습 전략의 중요성 🎯](#1-학습-전략의-중요성-🎯)
- [2. 활성화 함수 ⚡](#2-활성화-함수-⚡)
- [3. 학습 안정성 전략 🛡️](#3-학습-안정성-전략-🛡️)
- [4. 데이터 전처리 및 모델 상수 초기화 🧹](#4-데이터-전처리-및-모델-상수-초기화-🧹)
- [5. 모델 정규화 ✨](#5-모델-정규화-✨)

---

## 1. 학습 전략의 중요성 🎯

### 좋은 모델 구조 = 우수한 성능?
- **단순 구조만으로 성능 보장 X**
  - 학습 과정에서 불안정성, 과적합, 느린 수렴 문제 발생 가능

#### 자주 겪는 문제
| 문제 유형 | 설명 |
| -------- | ---- |
| ⚡ 학습 불안정 | 손실 폭발, 학습 멈춤, 수렴 실패 |
| 🔥 과적합 | 훈련 데이터에는 잘 맞지만 검증/실제 성능 저하 |
| 🐢 느린 수렴 | 최적점 도달까지 불필요한 에폭 소모, 학습 효율 저하 |

---

## 2. 활성화 함수 ⚡

### 2-1. 활성화 함수 개요
- 입력 신호 총합 → 출력 신호 변환
- **역할**  
  - 신경망에 비선형성 부여 → 복잡한 패턴 학습 가능
  - 활성화 함수 없으면 단순 선형 모델과 동일
- **중요성**: 학습 안정성과 성능에 큰 영향

### 2-2. 대표 함수 비교
| 함수 | 특징 | 단점 / 주의점 |
| ---- | ---- | ------------- |
| Sigmoid | 0~1 출력 | 입력 크기 극단적일 때 gradient 소실, 편향된 업데이트, 계산량 많음 |
| Tanh | -1~1 출력 | gradient 소실 가능, Sigmoid보다 중앙 0 기준으로 개선 |
| ReLU | max(0,x) | 간단, 효율적, gradient 소실 문제 감소 |
| Leaky ReLU | 음수 입력에도 작은 기울기 유지 | ReLU 단점 완화 |
| ELU | 음수 영역에 완만한 곡선 | 학습 안정성 증가, 계산량 조금 증가 |

> **Tip**: ReLU부터 시도하는 것이 일반적 ✅

---

## 3. 학습 안정성 전략 🛡️

### 3-1. 학습률 조정
- 학습률(Learning Rate)은 매우 중요!
- **계단식 변경(Step Decay)**, **Cosine Decay**, **Warmup** 등 다양한 방식 사용

### 3-2. 하이퍼파라미터 선정
- 최적 학습률, 배치 사이즈, 에폭 수 등 실험적 선정
- 작은 변경도 학습 안정성, 수렴 속도, 최종 성능에 큰 영향

---

## 4. 데이터 전처리 및 모델 상수 초기화 🧹

### 4-1. 데이터 전처리
- **형식 통일**: 입력 데이터 형태 맞추기
- 모델별 전처리 방식 고려 (예: 이미지 정규화, 텍스트 토큰화)

### 4-2. 모델 초기화 전략
| 전략 | 특징 | 장단점 |
| ---- | ---- | ------ |
| 0으로 초기화 | 모든 가중치 0 | 학습 불가 ❌ |
| 랜덤 초기화 | Gaussian 분포, 작은 값 | 깊은 모델에서는 gradient 폭발 위험 ⚠️ |
| Xavier 초기화 | 입력 차원 기반 분산 조정 | ReLU와 궁합 좋음, 출력 분산 안정 ✅ |
| He 초기화 | 2*입력 차원 기반 분산 | ReLU 계열 최적, 깊은 모델 안정 ✅ |

---

## 5. 모델 정규화 ✨

### 5-1. 가중치 감소(Weight Decay)
| 방식 | 특징 | 효과 |
| ---- | ---- | ---- |
| L2 (Ridge) | 가중치 제곱으로 패널티 | 큰 값 방지, 전체 균형 유지 |
| L1 (Lasso) | 가중치 절댓값으로 패널티 | 중요하지 않은 가중치 0 처리, 희소 모델 |
| Elastic Net | L1 + L2 결합 | 두 가지 장점 결합 |

### 5-2. 드롭아웃(Dropout)
- 학습 시 일부 뉴런 확률적으로 0 처리
- **장점**: 과적합 방지, 일반화 성능 향상, 앙상블 효과
- **단점**: 학습 시간 증가, 드롭 비율 최적화 필요, 모델마다 선호도 다름
