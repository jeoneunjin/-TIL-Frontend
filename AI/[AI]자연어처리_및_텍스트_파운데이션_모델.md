# 🤖 자연어 처리 및 텍스트 파운데이션 모델 학습 요약

## 📚 목차

---

## 1. 텍스트 파운데이션 모델

### 1-1. 텍스트 파운데이션 모델이란?

> = 거대 언어 모델

#### 텍스트 파운데이션 모델의 특이점

1. 규모의 법칙 : 더 많은 데이터, 큰 모델, 긴 학습 -> 더 좋은 성능
2. 창발성 : 특정 규모를 넘어서면 갑자기 모델에서 발현되는 성질

### 1-2. 거대 언어 모델 예시

#### 폐쇄형 vs 개방형 거대 언어 모델

| 종류   | 장점                                                               | 단점                                                                     | 예시                    |
| ------ | ------------------------------------------------------------------ | ------------------------------------------------------------------------ | ----------------------- |
| 폐쇄형 | 일반적으로 더 우수한 성능 및 쇠신 기능을 갖고 있으며 사용하기 쉬움 | 사용할 때마다 비용이 발생, 모델이나 출력에 대한 정보가 제한적으로 제공됨 | ChatGPT, Claude, Gemini |
| 개방형 | 무료로 다운로드 및 사용 가능, 모든 정보가 공개되어 있음            | 충분한 계산 자원 필요, 상대적으로 폐쇄형 모델에 비해 성능이 낮은 편      | LLaMA, Gemma, Qwen      |

---

## 2. 거대 언어 모델의 학습

#### 정렬 학습

> 거대 언어 모델의 출력이 사용자의 의도와 가치를 반영하도록 하는 것

- 지시 학습 : 주어진 지시에 대해 어떤 응답이 생성되어야 하는지
- 선호 학습 : 상대적으로 어떤 응답이 더 선호되어야 하는지

### 2-1. 지시학습(Instruction Tuning)

#### 📘 기본 개념

- 기존 언어 모델(BERT 등)의 **지도 추가 학습(Supervised Fine-Tuning, SFT)** 방식과 유사
- 하지만 단순히 “입력–정답”이 아닌  
  👉 **자연어 형태의 ‘지시(Instruction)’와 ‘응답(Response)’ 쌍**으로 학습

#### ⚙️ 기존 방식의 한계

|   구분    | 기존 언어 모델              | 지시학습 모델                                |
| :-------: | :-------------------------- | :------------------------------------------- |
| 학습 방식 | 태스크별 별도 SFT 필요      | 하나의 통합 모델로 학습                      |
| 입력 형태 | 구조화된 태스크 입력        | 자연어 지시문                                |
|  확장성   | 낮음 (테스크마다 모델 필요) | 높음 (지시만 바꾸면 새로운 태스크 수행 가능) |

#### 💡 새로운 접근

> “모든 자연어 테스크를 텍스트 기반의 **지시 + 응답** 형태로 표현할 수 있지 않을까?”

- 거대 언어 모델을 다양한 **지시–응답 쌍 데이터**로 추가 학습  
  → 대표 사례: **FLAN (Fine-tuned LAnguage Net)**
- 한 테스크를 여러 지시 형태로 표현하여 **데이터 다양성** 확보  
  예시:
  - “이 문장을 영어로 번역해줘.”
  - “Translate this sentence into English.”

#### 🌈 기대 효과

- **다양한 지시 표현**을 학습 → **일반화 성능 향상**
- **예시 없이도(0-shot)** 새로운 지시에 **정확한 응답 생성 가능**
- 하나의 모델로 **다양한 언어 테스크 수행 가능**

#### 🧪 실험 결과: 성능 향상의 핵심 요소

| 요소                 | 설명                                                                                  |
| :------------------- | :------------------------------------------------------------------------------------ |
| ① 학습 테스크의 개수 | 다양한 종류의 지시를 학습할수록, 보지 못한 지시에 대한 일반화 성능이 향상             |
| ② 모델의 크기        | 일정 규모 이상의 모델에서 지시학습의 효과가 뚜렷하게 나타남 (소형 모델은 효과 제한적) |
| ③ 지시의 표현 방식   | **자연어 형태로, 사람에게 대화하듯 지시하는 방식**이 가장 효과적                      |

---

### 💭 2-2. 선호 학습(Preference Learning)

#### 📘 기본 개념

> 다양한 응답 중 **사람이 더 선호하는 응답**을 생성하도록 모델을 추가 학습하는 과정  
> 다양한 응답은 **모델이 생성**, 응답 간의 **선호도는 사람이 제공**

#### ⚠️ 지시학습의 한계

> 지시학습은 “주어진 입력에 대해 **하나의 정답이 존재**한다”고 가정

- ✅ 정답이 명확한 **객관적 테스크(예: 번역, 분류)**에서는 적합
- ❌ 하지만 정답이 고정되지 않은 **개방형(Open-ended) 테스크**에서는 한계 존재  
  → 즉, “정답이 하나가 아닐 때”, 사람이 **더 자연스럽게 느끼는 응답**을 학습시킬 필요가 있음  
  👉 이를 가능하게 하는 것이 **선호 학습(Preference Learning)**

#### 🤖 예시: InstructGPT의 학습 과정

InstructGPT는 아래의 **3단계 학습 절차**로 이루어짐 👇

| 단계 | 과정                                                            | 핵심 내용                                                                                                                                                                                                                         |
| :--: | :-------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|  ①   | **지시 학습(SFT)**                                              | - 실제 유저의 다양한 지시 입력 수집<br>- 훈련된 주석자(Annotator)가 정답 데이터를 생성<br>→ 텍스트 파운데이션 모델을 지시 기반으로 추가 학습                                                                                      |
|  ②   | **보상 모델 학습 (Reward Model)**                               | - 모델이 여러 응답 후보를 생성<br>- 사람은 그중 더 선호하는 응답을 선택<br>- 사람이 선택한 순위를 바탕으로 **보상 모델** 지도 학습<br>→ 사람이 선호하는 응답일수록 **높은 보상값** 출력                                           |
|  ③   | **강화 학습(RLHF: Reinforcement Learning from Human Feedback)** | - 보상 모델의 신호를 이용해, 모델이 높은 보상을 받도록 **정책(Policy)**을 업데이트<br>- 새로운 입력에 대해 사람의 개입 없이도 **사람이 선호할 만한 응답** 생성 가능<br>- 즉, SFT 모델을 보상 모델 기반 **강화학습으로 추가 개선** |

#### 🌈 기대 효과

- 사람이 직접 평가한 선호도를 반영 → **더 자연스럽고 인간 친화적인 응답** 생성
- **정답이 명확하지 않은 상황에서도** 품질 높은 응답 제공
- 이후 GPT 시리즈(예: ChatGPT)는 이 방식을 바탕으로 발전

#### 🏁 요약

> 선호 학습은 “정답이 하나”라는 한계를 넘어  
> **“사람이 더 좋아하는 응답을 학습하는 과정”**으로  
> 모델을 인간 중심적으로 진화시킨 핵심 단계

---

## 3. 거대 언어 모델의 추론

### 3-1. 디코딩 알고리즘

#### 거대 언어 모델의 자동회귀 생성

- 학습이 완료된 거대 언어 모델은 어떻게 응답을 생성할까? -> 순차적 추론을 통한 토큰별 생성
  ? 언제 추론 및 토큰 생성을 멈추고 응답 제공

  > EOS 토큰 생성 시 종료 or 사전에 정의된 토큰 수 도달 시 종료

- 목표 : 주어진 입력($x=[x_1,x_2,...,x_L]$)에 대해 다음 토큰($x_{L+1}$) 생성
- 디코딩 알고리즘 입력 모델 $x$에 대해 다음 토큰에 대한 확률 분포 $hat\p(x)$ 로부터 ${x_{L+1}}$ 을 생성하는 알고리즘(다음 단어를 선택하는 방법)

#### 디코딩 알고리즘

1. Greedy Decoding

- 핵심 아이디어 : 가장 확률이 높은 다음 토큰을 선택(Greedy Decoding)
  - 장점 : 사용하기 쉽다
  - 단점 : 직후만 고려하기 때문에 생성 응답이 최종적으로 최선이 아닐 수 있다.

2. Beam Search

- 핵심 아이디어 : 확률이 높은 k개(beam size)의 후보를 동시에 고려
  - 고르는 기준 : 누적 생성 확률(지금까지 생성한 문장 전체가 나올 확률의 곱)
  - 앞선 1번 방법은 매 시점마다 가장 높은 확률의 선택지 1개만 선택, 해당 방법은 전체 문장 후보들의 누적 확률을 기준을 상위 k개 남기는 것

3. Sampling

- 핵심 아이디어 : 거대 언어 모델이 제공한 확률을 기준으로 랜덤하게 생성
  - 장점 : 다양한 응답을 생성할 수 있음
  - 단점 : 생성된 응답의 품질이 감소할 수 있음

4. 기존 Sampling 보안 - Sampling with Temperature

- 핵심 아이디어 : 하이퍼 파라미터 T를 통해 거대 언어 모델이 생성한 확률 분포를 임의로 조작
  - T>1 : 확률 분포를 Smooth하게 만듦(더 다양한 응답 생성) T<1 : 확률 분포를 sharp하게 만듦(기존에 확률이 높은 응답에 집중)

5. Top-K Sampling

- 핵심 아이디어 : 확률이 높은 K개의 토큰들 중에서만 랜덤하게 확률에 따라 샘플링
  - 장점 : 품질이 낮은 응답을 생성할 가능성을 줄일 수 있음
  - 단점 : 확률 분포의 모양에 상관 없이 고정된 K개의 후보군을 고려

6. Top-P Sampling(or Nucleus Sampling)

- 핵심 아이디어 K개를 고정하는 대신, 누적 확률 P에 집중하여 K를 자동으로 조절
- 다양한 평가 지표에서 기존 딬딩 알고리즘들 대비 좋은 성능을 달성

---

### 3-2. 프롬프트 엔지니어링

> 원하는 답을 얻기 위해 모델에 주어지는 입력(프롬프트)을 설계-조정하는 기법

- 어떻게 지시를 주는지 어떤 예시를 보여주는지가 거대 언어 모델의 성능에 크게 영향을 미침

#### Chain-of_Thought(CoT) 프롬프팅

- 아이디어 : 단순히 질문과 응답만을 예시로 활용하는 것이 아니라, 추론 과정도 예시에 포함

  - 이를 통해, 테스트 질문에 대해 추론을 생성하고 응답하도록 유도함으로써, 더 정확한 정답 생성을 기대할 수 있음

- 질문에 대한 정답을 바로 제시 -> 틀림
- 질문에 대한 정답이 나오는 추론 과정을 함께 제시 -> 정답률 상승

- 결과 : CoT는 거대 언어 모델(PaLM)의 추론 성능을 크게 증가시킴
- Cot로 인한 성능 향상은 모델 크기가 커질 수록 더 확대됨

- 한계 : 예시를 위한 추론 과정을 수집해야 되는 문제가 있음

#### 0-shot CoT 프롬프팅

1. 유인 문장을 통한 추론 생성(예시 없음)
2. 추출 문장을 통한 추론 생성
3. 주어진 질문과 생성된 추론을 통한 정답 생성

- 결과 :
  - 0-shot CoT 는 기존 0-shot 프롬프팅보다 훨씬 높은 추론 성능을 달성
  - 모델 크기가 임계점을 넘어서야 효과성이 발휘
  - 단순한 문구 하나가 성능을 크게 향상시키거나 성능을 떨어뜨림(추출 문장의 중요성!)

---

## 4. 거대 언어 모델의 평가와 응용

### 4-1. 거대 언어 모델의 평가

#### 평가

> 구축한 시스템(ex. 코드 or 앱)이 실제로 잘 동작하는지를 확인하는 단계

- 평가의 3가지 요소

1. 목표 : 시스템으로 무엇을 달성하고자 하는지
2. 평가 방법 : 어떤 방법으로 평가?
3. 평가 지표 : 어떻게 성공 여부를 판단?

#### AI 모델의 평가 - 테스트 데이터

- 핵심 가정: 학습 단계에서 본 적이 없고, 질문과 정답을 알고 있음
- 예시 - 감정분류
  1. 데이터 입력 : "영화는 정말 형편없었어요"
  2. 추가 학습된 언어 모델
  3. 예측 감정 = 정답 감정(예측과 정답 비교)

#### 거대 언어 모델 평가의 특징

- 특정 테스크에서 학습된 기존 AI 모델들과 달리, 거대 언어 모델은 다양한 테스크에 대해 동시에 학습됨

#### 거대 언어 모델 평가 방법의 종류

- 정답이 정해진 경우 :
  - 평가 방법 : 에측과 정답을 비교하여 일치도를 측정
- 정답이 정해져 있지 않은 경우
  - 방법 1 : 사람이 임의의 정답을 작성 및 이와 예측을 비교
  - 방법 2 : 정답과 무관하게 생성 텍스트 자체의 품질만을 측정
  - 방법 3 : 생성된 텍스트의 "상대적 선호"를 평가

#### 거대 언어 모델을 활용한 평가

- LLM-as-judge(or G-Eval) :
  - 거대 언어 모델을 통해 생성 텍스트를 평가
  - 유저는 풀고자하는 테스크, 평가하고자 하는 텍스트, 평가 기준을 제공
  - 거대언어모델은 평과 결과(점수, 이유) 제공
- GPT-4
- LLaMA3 학습을 위한 데이터 필터링에도 사용되는 등 다양한 어플리케이션에서 이미 활용되고 있음

---

### 4-2. 거대 언어 모델의 응용 및 한계
