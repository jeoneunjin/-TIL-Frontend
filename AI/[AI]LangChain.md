# 🤖 LangChain 학습 요약

## 📚 목차

---

## 1. Instruction-tuning

- 사전 학습된 거대 언어 모델은 **유저 의도와 일치하지 않는 경우가 많음** → 파인튜닝 필요

### 🔹 파인튜닝 (Fine-tuning)

- 이미 사전 학습된 모델을 **특정 작업/도메인**에 맞춰 추가 학습
- 목표: 모델이 실제 사용 환경에 맞춰 **정확하고 적절한 답변** 생성

---

### 1-1. Instruction-tuning

- **목적:** 언어 모델이 **사람의 지시문(instruction)**을 따르도록 학습
- 정답 레이블 필요, 다양한 태스크에 적응 가능
- 과정:
  1. 다양한 태스크에서 **(지시문, 응답) 쌍** 수집
  2. 사전학습 모델에 파인튜닝 적용
  3. 새로운 태스크에서 모델 평가 수행

#### ⚠️ 한계

| 한계             | 설명                                                           |
| ---------------- | -------------------------------------------------------------- |
| 정답 부재        | 개방형/창의적 생성 태스크는 정답이 없음                        |
| 오류 평등 처리   | 모든 토큰 수준 오류를 동일하게 취급 → 실제 영향력 다를 수 있음 |
| 레이블 최적 아님 | 사람이 만든 정답이 항상 최적답은 아님                          |

---

## 2 RLHF (Reinforcement Learning with Human Feedback)

> 인간 선호를 반영하여 언어 모델 **응답의 기대 보상(expected reward) 최대화**

### 🔹 파이프라인

1. **시범 데이터 수집** → 감독 학습(Supervised Fine-tuning)
2. **비교 데이터 수집** → **리워드 모델(Reward Model, RM)** 학습
3. **강화학습(PPO 등)** → 정책 최적화

---

### 🔹 인간의 선호도 모델링

- 문제: 인간 평가 **일관성 부족**, 기준이 다를 수 있음
- 해결: **점수 대신 비교**
  - “답변 A와 B 중 어느 쪽이 더 좋나요?”
  - RM 학습 시 직접 점수보다 **상대적 선호** 활용

---

### 🔹 리워드 모델 한계

| 한계                    | 설명                                                  |
| ----------------------- | ----------------------------------------------------- |
| 인간 선호 일관성 부족   | 평가자 간 편차 존재                                   |
| 리워드 해킹(RM Exploit) | 모델이 정답 여부와 상관없이 **좋아 보이는 답변 생성** |
| 환각(Hallucination)     | 선호 기반 학습이 오히려 **허구 정보 생성** 유발       |

💡 **요약**

- Instruction-tuning: 모델이 지시를 따르도록 **정답 중심 파인튜닝**
- RLHF: 모델이 사람 선호를 따르도록 **리워드 기반 최적화**
- 한계:
  - Instruction-tuning → 정답 없거나 최적 레이블 어려움
  - RLHF → RM 학습 불안정, 환각 가능성

---

## 3. 다음 대안책?

### 3-1. DPO(Direct Preference Optimization)

> RLHF에서 'RL'을 제거하자

#### RLHF vs DPO: 상황 및 핵심 이해

#### 1️⃣ 상황 정리

**사전학습된 모델(Pretrained Model)**

- 대량 텍스트로 학습됨
- 일반적인 언어 능력은 뛰어나지만, **특정 목표(예: 유저가 좋아하는 답변 생성)** 달성에는 부족

**선호도 모델(Reward Model, RM)**

- 사람 피드백 기반 학습
- “이 응답이 더 좋은가?” 판단
- 사전학습 모델이 문법/지식만 보는 것과 달리, 사람 선호 스타일과 목표 달성을 반영

**RLHF (Reinforcement Learning with Human Feedback)**

- RM 피드백을 기반으로 **정책(policy)** 학습
- 문제: RL 과정 복잡, 학습 불안정, 일부 경우 일관성 없는 답변 발생 가능

#### 2️⃣ RL을 뺀다는 의미

- RLHF에서 RL 부분 제거 → **DPO(Direct Preference Optimization)**
- RL 없이, RM 결과를 **직접 최대화**하도록 학습
- 샘플링/보상 기반 업데이트 없이 선호도 직접 최적화
- 장점: RL보다 학습 안정적, 복잡하지 않음

#### 3️⃣ 핵심: “선호도 분석을 뺀다?”

- DPO는 **선호도 모델 자체는 유지**
- RL 과정만 제거, 직접 최적화 방식으로 학습
- 비유:
  - RLHF: “선호도 점수 보고, 점수 최대화 위해 여러 답변 시험하며 학습”
  - DPO: “선호도 점수 보고, 바로 가장 좋아할 답변으로 최적화”
- 즉, **선호도 모델은 그대로, RL 단계만 제거**

---

## 4. 검색증강 언어모델(Retrieval-augmented LM)

### 4-1. 검색증강 언어모델이란?

- 추론 시 외부 데이터 저장소를 불러와 활용하는 언어모델
- 사용자의 질문에 답하기 위해, datastore에서 관련 정보를 검색(Retrieval) 해와서, 이를 언어모델이 생성 단계에 활용하는 방법

- 구성 요소 :

  > Datastore, Query, Index, language Model

- Datastore:
  > 가공되지 않은 대규모 텍스트 코퍼스
  - 최소 수십억에서 수조 단위의 토큰으로 구성
  - 라벨링된 데이터셋 또는 지식 베이스와 같은 구조화된 데이터가 아님
- Query:
  > 검색 질의/ Retrieval input
  - 언어모델의 질의와 같아야하는 것은 아님
- Index:
  > 문서나 단락과 같은 검색 가능한 항목들을 체계적으로 정리하여 더 쉽게 찾을 수 있도록 하는 것
  - 각 정보 검색 메서드는 인덱싱 과정에서 구축된 인덱스를 활용해, 퀄리와 관련 있는 정보를 식별

### 4-2. Information Retrieval

- 사용자의 질의(Query)에 맞는 정보를 대규모 데이터에서 찾아 제공하는 과정
- 목표 : 검색 질의와 가장 관련성 높은 정보 제공
  > 질문 -> 관련 정보 검색 -> 검색 결과 제공

#### 정보검색(IR)의 활용

- 웹 서치 & 아이템 서치
- 추천 시스템
  - OTT 서비스
  - 이커머스
- 검색증강생성(RAG)
  - 검색된 문서를 활용하여 더 정확하고 최신의 답변 생성

#### 종류

- Sparse Retriever(어휘적 유사도 기반)
  - 전통적인 정보검색 기법, 쿼리와 문서 간의 정확한 용어 일치에 기반
  - Ex. TF-IDF, BM-25
- Dense Retriever(의미적 유사도 기반)
  - 쿼리와 문서를 표현하기 위해 dense vector을 활용해 의미적 유사도에 기반
  - Ex. DPR, Contriever, Openai-embeddings, etc

---

## 5. Retrieval-augmented LM

### 5-1. Retrieval-augmented LM이란?

- 추론 시 외부 데이터 저장소를 불러와 홀용하는 언어모델 = RAG
- RAG(Retrieval-augmented Generation): 정보 검색부터 답변 생성까지의 프레임워크

#### 사용 이유?

1. 거대 언어 모델은 모든 지식을 다 자신의 파라미터에 저장하지 못함 - 거대 언어 모델은 사전학습 데이터에 자주 나타나는 쉬운 정보를 기억하는 경향성이 있음 이 과정에서 정보의 왜곡, 손실 등이 발생할 수 있다. - RAG는 자주 등장하지 않는 정보에 대해서 큰 효과를 가져다 줌
   -> 자주 등장하지 않는 정보의 경우가 실생활에 더 많기 때문에 RAG가 성능이 더 좋다?
2. 거대 언어 모델이 보유한 지식은 금세 시대에 뒤쳐지며, 갱신이 어려움
   - 현재의 지식 편집 메서드들은 확장성이 부족
   - 반면, 저장소(Datastore)는 쉽게 업데이트가 가능, 확장성도 만족
3. 거대 언어 모델의 답변은 해석과 검증이 어려움
4. 기업 내부 정보와 같은 보안 정보는 언어모델 학습에 활용되지 않음
   - 사내 챗봇/기업 내부 시스템에 언어모델을 사용하는 경우 내부 데이터를 학습 시 정보 유출의 위험성이 있음

#### 파이프라인

- 검색증강 언어모델 ; 언어모델에 질문과 더불어 검색엔젠 결과를 함께 이용

1. 질의 추출
2. 문서 검색
3. 언어모델 추론

#### 한계

- RAG의 결과는 검색 모델 성능에 의존 -> 검색 노이즈에 취약 -> Hallucination 증가
  > 즉, 정보 검색을 사용하는 것만으로는 사실의 정확성 보장 X
- LLM의 사전지식과 컨텍스트 간의 충돌 발생

---

## 6. LLM Agents

### 6-1. Agent란?

- 센서를 통해 환경을 인지하고, 액추에이터를 통해 액션이 환경에 영향을 주는 것으로 간주될 수 있는 모든 것

### 6-2. LLM Agents?

- 거대 언어 모델을 핵심 구조로 삼아 환경을 이해하고 행동을 수행하는 에이전트
- LLM-first view : 기존 LLM을 활용한 시스템을 에이전트로 만듦
- Agent-first vies : LLM을 AI 에이전트에 통합하여, 언어를 활용한 추론과 의사소통을 가능하게 함

#### 이것은 에이전트인가?

- 웹을 탐색하는 LLM 시스템 - Yes
- OS의 파일을 검색하고 코드를 사용해 처리하는 LLM 시스템 - Yes
- 정보를 검색하고 생성하는 LLM 시스템 - Probably not(점진적이지 않음)
- 복자반 추론을 하는 O1같은 LLM - No(툴도 없고 외부 환경과 상호작용하지 않음)

#### 성공적인 에이전트가 갖추어야 할 요건들

- 도구 사용
- 추론과 계획
- 환경 표현
- 환경 이해
- 상호작용/의사소통

### 6-3. Tool Usage in LLMs

#### Tool이란?

- 언어 모델 외부에서 실행되는 프로그램에 연결 되는 함수 인터페이스를 의미
  - LLM은 함수 호출과 입력 인자를 생성함으로써 이 도구를 활용할 수 있음

#### Tool Learning 빙식

- 모방 학습 : 인간의 도구 사용 행동 데이터를 기록함으로써, 언어 모델이 인간의 행동을 모방하도록 학습
  - Meta : TOllformer :
  - 모델 스스로 학습 데이터 생성
  - 지도 학습
  - 검색 엔진 뿐만 아니라 달력, 계산기와 같은 여러 API 사용
  - ToolLLM
- 멀티 모달 툴 러닝 : 멀티모달 대규모 언어모델(MLLM)을 기반으로 도구를 정의하고 활용하는 연구
- 강화 학습

---

## 7. MCP(Model Context Protocol)

- 언어 모델이 외부 툴과 상호작용하기 위한 표준화된 방식으로 정의한 프로토콜
- 툴 호출, 응답 전달, 컨택스트 공유를 하나의 공통 규격으로 처리
